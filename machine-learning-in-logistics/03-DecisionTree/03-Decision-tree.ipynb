{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __ML in der Logistik__\n",
    "# Exercise 03 - Decision trees\n",
    "\n",
    "__Goal__: understand the algorithms decision tree, the concept of pruning, build a simple decision tree  algorithm in Python and use pruning.\n",
    "\n",
    "\n",
    "__Data and application (Task 1.2)__: The dataset contains the Length, Duration and Route informations from AIS messages recorded on 2701 trips on 4 routes in North Europa (Bremerhaven > Hamburg, Felistowe > Rotterdam, Kiel > Gdynia, Rotterdam > Hamburg). We want to build a decision tree to predict the route using the other attributes.\n",
    "\n",
    "__Data and application (Tasks 1.3 and 1.4, Part 2)__: The dataset contains the static information of 4503 trips recorded on 4 routes in North Europa (Rotterdam > Hamburg, Kiel > Gdynia, Bremerhaven > Hamburg, Felixstowe > Rotterdam). We want to build and prune a decision tree predicting if the trip was direct or made additional stops.\n",
    "\n",
    "\n",
    "\n",
    "### Contents\n",
    "\n",
    "* [1 Theory: understand the algorithm](#t1)\n",
    "    * [1.1 The decision tree algorithm (20 min)](#t11)\n",
    "    * [1.2 Drawing a tree (10 min)](#t12)\n",
    "    * [1.3 Pruning (15 min)](#t13)\n",
    "* [2 Practice: predict additional stops for a trip](#t2)\n",
    "    * [2.1 The DecisionTreeClassifier algorithm (20 min)](#t21)\n",
    "    * [2.2 Cost Complexity Pruning (20 min)](#t22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Theory: understand the algorithm <a class=\"anchor\" id=\"t1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 The decision tree algorithm (20 min) <a class=\"anchor\" id=\"t11\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Quickly explain the decision tree algorithm.*\n",
    "\n",
    ": Supervised algorithm which can be used in classification and regression, this works by splitting the given data into subsets recursively based on the attribute values, and forming a tree.\n",
    "\n",
    "*How does a decision tree handle continuous numeric attributes?*\n",
    "\n",
    ": Sorts and choses based on threshold which gives an optimal tree (using Information Gain, Gini Index etc..)\n",
    "\n",
    "*How can a tree handle a regression problem?*\n",
    "\n",
    ": Using variance reduction or MSE instead of information gain or gini index, tree can be used for regression problem where leafs contains average of target values [piece wise constant approximation]\n",
    "\n",
    "*How does the decision tree algorithm select an attribute to split on, for each node?*\n",
    "\n",
    ": Information Gain (ID3 - Iterative Dichotomiser 3), Gini Index (CART - Classification and Regression Trees), Gain Ratio (C4.5), Variance Reduction (CART)\n",
    "\n",
    "*How can a decision tree handle missing values for an attribute, in the learning and in the prediction phase?*\n",
    "\n",
    ":\n",
    "- Learning: can be ignored or can use weighted split based on how other instances with known values are distributed\n",
    "- Prediction: can follow multiple paths proportionally or can use the most frequent path based on the learning data (remember weights)\n",
    "\n",
    "*Is a decision tree robust to outliers? Why?*\n",
    "\n",
    ": Somewhat robust, cause split is based on threshold the effect of an outlier is less severe.\n",
    "\n",
    "*Explain the strategies of prepruning and postpruning.*\n",
    "\n",
    ": \n",
    "\n",
    "- Prepruning: Stop growing if next split doesn't perform well, stops overfitting early but may stop soon (underfitting)\n",
    "- Postpruning: Grow the full tree first and then cut the subtrees which doesn't perform well for accuracy (usually gives better generalization than prepruning)\n",
    "\n",
    "*What is the result of each one of these two approaches for building a tree?*\n",
    "+ *Select few big clusters, even if some instances are misclassified*\n",
    ": Shallow nodes, simpler model and possibly underfits (prepruning)\n",
    "+ *Select a lot of small clusters perfectly defining one class*\n",
    ": More nodes, complex model and possibly overfits (no pruning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Drawing a tree (10 min) <a class=\"anchor\" id=\"t12\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are in possession of a dataset which contains the values for Length, Duration and Route for 2701 trips. We want to build a decision tree to classify the attribute Route. The dataset is represented on the following graph:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![text](03-building.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_According to the graph, which attribute should be selected for the first node of the tree? What would be the value of splitting for this node?_\n",
    "\n",
    ": Duration and value would be 1000\n",
    "\n",
    "_Draw a decision tree with a maximum depth of 4 which would classify this dataset (not perfect classification)._\n",
    "\n",
    "![image.png](decision-tree-not-perfect-classification.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Pruning (15 min) <a class=\"anchor\" id=\"t13\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are given the following dataset. It contains the static data for 4503 trips on the 4 usual routes, and a new attribute 'Direct' which states if the ship took a direct trip from the start port to the end port, or if it took additional stops in between. A value of 1 indicates that the trip was direct, while a value of 0 indicates one or more additional stops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   shiptype  Length  Breadth Draught        SOG  TotalDistance  Duration  \\\n",
      "0        71     269       32   10.64  11.830764     353.861375      1532   \n",
      "1        71     323       43      12  13.203446     353.416043      1450   \n",
      "2        71     376       48   13.23  14.779070     346.753233      1213   \n",
      "3        71     300       49   12.83  13.668775     350.611945      1327   \n",
      "4        70     145       25       ?  12.389986     369.674767      1535   \n",
      "\n",
      "     Route  Direct  \n",
      "0  ROT_HAM       1  \n",
      "1  ROT_HAM       1  \n",
      "2  ROT_HAM       1  \n",
      "3  ROT_HAM       1  \n",
      "4  ROT_HAM       1  \n",
      "['ROT_HAM' 'KIE_GDY' 'FEL_ROT' 'BRE_HAM']\n"
     ]
    }
   ],
   "source": [
    "# Import and visualize the dataset\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('03-decision-tree_1.csv')\n",
    "print(df.head())\n",
    "print(df['Route'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this dataset, we want to build a decision tree that predicts the value of the attribute 'Direct'.\n",
    "\n",
    "Following is a decision tree that has been built using the software Weka. We want to study the effect of pruning a decision tree.\n",
    "\n",
    "Note 1: The tree being very complex, the right part of it has been cut for visualization. We focus here on the left part only (instances with TotalDistance <= 399)\n",
    "\n",
    "Note 2: Each circle represents a node of the tree, each square is a leaf. 'Direct' stands for a classification with the value for the attribute 'Direct' = 1, Not direct is 'Direct' = 0. The values between parentheses stand for: (number of correctly classified instances / number of uncorrectly classified instances)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![text](03-pruning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_What happens to a subtree which is pruned?_\n",
    "\n",
    "__For each subtree, write down the repartition of the instances on the form: Direct-TP (true positive), Direct-FP (false positive), Not direct-TN (true negative), Not direct-FN (false negative). Write down your answers in the following code cell. (3 points)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition of the instances\n",
    "\n",
    "# Subtree1\n",
    "t13_TP_1 = 4026\n",
    "t13_FP_1 = 11\n",
    "t13_TN_1 = 25\n",
    "t13_FN_1 = 6\n",
    "\n",
    "# Subtree2\n",
    "t13_TP_2 = 14\n",
    "t13_FP_2 = 0\n",
    "t13_TN_2 = 21\n",
    "t13_FN_2 = 0\n",
    "\n",
    "# Subtree3\n",
    "t13_TP_3 = 33\n",
    "t13_FP_3 = 1\n",
    "t13_TN_3 = 3\n",
    "t13_FN_3 = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Then, calculate the error rate that each subtree would give if it were transformed into a single leaf, as: error rate = (number of instances in the minimal class) / (total number of instances). Write down your answer in the following code cell. (3 points)__\n",
    "\n",
    "\n",
    "here minority is class (target) with minimum cardinality in that particular subtree, (TP + FN - positive class, TN + FP - negative class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error rate\n",
    "\n",
    "# Subtree1\n",
    "t13_error_1 = 36/4068\n",
    "\n",
    "# Subtree2\n",
    "t13_error_2 = 14/35\n",
    "\n",
    "# Subtree3\n",
    "t13_error_3 = 4/37"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__From the error rates: which subtree should be pruned first? Write down your answer in the following code cell (the variable pruned should be an integer and equal to the number of the subtree you choose to prune). (2 points)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruned subtree\n",
    "\n",
    "t13_pruned = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_What can we say about pruned trees?_\n",
    "\n",
    ": since error rate is low we have pruned the tree, here it seems by visualizing most of the trips belong to direct rather than not direct and hence we can prune this and just classify as direct at the pruning node itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_What other methods can be used to prune a tree?_\n",
    "\n",
    ": Subtree raising (deleting the inner node) and Cost complexity pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Practice: predict additional stops for a trip <a class=\"anchor\" id=\"t2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 The DecisionTreeClassifier algorithm (20 min) <a class=\"anchor\" id=\"t21\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task, we will use the dataset that we saw in tasks 1.3 and 1.4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('03-decision-tree_1.csv', na_values = '?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For building a decision tree, we need all the attributes to be numerical. Therefore, we won't use the attribute 'Route' for this prediction. Also, the attribute 'Direct' should be categorical.\n",
    "\n",
    "__Change the type of the attribute.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the type of the attribute\n",
    "df.drop(\"Route\", axis = 1)\n",
    "df['Direct'] = df['Direct'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to build a simple decision tree model to predict the attribute 'Direct' from the static information of the trips. We will use the classifier ``DecisionTreeClassifier`` from the ``sklearn`` library. This classifier may use random values, so in this exercise we will specify the value for the parameter ``random_state = 1``.\n",
    "\n",
    "The ``DecisionTreeClassifier`` can't process the attributes with missing values. If there are some in the dataset, identify them.\n",
    "\n",
    "We want to predict the attribute 'Direct' using all the other attributes except 'Route' and any attribute with missing values.\n",
    "\n",
    "For training and testing, we will use a 80-20% split.\n",
    "\n",
    "__Write the code to apply the algorithm. Do not specify any parameter for the algorithm else than__ ``random_state = 1`` __and compute the accuracy of the model built.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Draught']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.columns[df.isnull().any()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97669256381798\n"
     ]
    }
   ],
   "source": [
    "# Apply the DecisionTreeClassifier algorithm\n",
    "\n",
    "df.drop(\"Draught\",axis=1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "x = ['shiptype', 'Length', 'Breadth', 'SOG', 'TotalDistance', 'Duration']\n",
    "y = ['Direct']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[x], df[y], test_size = 0.2, random_state = 1)\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=1)\n",
    "dt.fit(X_train, y_train)\n",
    "predictions = dt.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier ``DecisionTreeClassifier`` have several parameters that may need tuning. One of them is ``max_depth``, representing the depth of the tree.\n",
    "\n",
    "_What would be the consequence of a tree with unlimited depth?_\n",
    "\n",
    ": Might overfit\n",
    "\n",
    "__Take your code from the previous task, and try out a couple of values for the parameter__ ``max_depth``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Depth: 1, Accuracy: 0.9722530521642619\n",
      "Max Depth: 2, Accuracy: 0.9722530521642619\n",
      "Max Depth: 3, Accuracy: 0.9800221975582686\n",
      "Max Depth: 4, Accuracy: 0.9822419533851277\n",
      "Max Depth: 5, Accuracy: 0.9833518312985572\n",
      "Max Depth: 6, Accuracy: 0.9833518312985572\n",
      "Max Depth: 7, Accuracy: 0.9811320754716981\n",
      "Max Depth: 8, Accuracy: 0.9811320754716981\n",
      "Max Depth: 9, Accuracy: 0.978912319644839\n",
      "Max Depth: 10, Accuracy: 0.9778024417314095\n",
      "Max Depth: 11, Accuracy: 0.978912319644839\n",
      "Max Depth: 12, Accuracy: 0.978912319644839\n",
      "Max Depth: 13, Accuracy: 0.97669256381798\n",
      "Max Depth: 14, Accuracy: 0.9778024417314095\n",
      "Max Depth: 15, Accuracy: 0.97669256381798\n",
      "Max Depth: 16, Accuracy: 0.97669256381798\n",
      "Max Depth: 17, Accuracy: 0.97669256381798\n",
      "Max Depth: 18, Accuracy: 0.97669256381798\n",
      "Max Depth: 19, Accuracy: 0.97669256381798\n",
      "Max Depth: 20, Accuracy: 0.97669256381798\n",
      "Max Depth: 21, Accuracy: 0.97669256381798\n",
      "Max Depth: 22, Accuracy: 0.97669256381798\n",
      "Max Depth: 23, Accuracy: 0.97669256381798\n",
      "Max Depth: 24, Accuracy: 0.97669256381798\n",
      "Max Depth: 25, Accuracy: 0.97669256381798\n",
      "Max Depth: 26, Accuracy: 0.97669256381798\n",
      "Max Depth: 27, Accuracy: 0.97669256381798\n",
      "Max Depth: 28, Accuracy: 0.97669256381798\n",
      "Max Depth: 29, Accuracy: 0.97669256381798\n",
      "Max Depth: 30, Accuracy: 0.97669256381798\n"
     ]
    }
   ],
   "source": [
    "# Try different values for max_depth\n",
    "def max_depth_dt(max_depth:int = None):\n",
    "\tdt = DecisionTreeClassifier(random_state=1, max_depth=max_depth)\n",
    "\tdt.fit(X_train, y_train)\n",
    "\tpredictions = dt.predict(X_test)\n",
    "\taccuracy = accuracy_score(y_test, predictions)\n",
    "\tprint(f\"Max Depth: {max_depth}, Accuracy: {accuracy}\")\n",
    "\n",
    "for depth in range(30):\n",
    "\tmax_depth_dt(depth + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the value for ``max_depth`` which gives the best accuracy? __Write it down in the following cell (if several values give the maximum accuracy, just write one of them). (1 point)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best value for max_depth (integer)\n",
    "\n",
    "t21_max_depth = 5 #Accuracy: 0.9833518312985572"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter ``min_samples_leaf`` gives the minimal number of instances required for a leaf node.\n",
    "\n",
    "_What would be the consequence of a tree where the leaves can hold 1 instance?_\n",
    "\n",
    ": Might overfit\n",
    "\n",
    "__Take your code from the previous task, and try out a couple of values for the parameter__ ``min_samples_leaf``. Do not specify a value for ``max_depth``, keep the default parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Samples Leaf: 1, Accuracy: 0.97669256381798\n",
      "Min Samples Leaf: 2, Accuracy: 0.978912319644839\n",
      "Min Samples Leaf: 3, Accuracy: 0.978912319644839\n",
      "Min Samples Leaf: 4, Accuracy: 0.978912319644839\n",
      "Min Samples Leaf: 5, Accuracy: 0.9822419533851277\n",
      "Min Samples Leaf: 6, Accuracy: 0.9800221975582686\n",
      "Min Samples Leaf: 7, Accuracy: 0.9822419533851277\n",
      "Min Samples Leaf: 8, Accuracy: 0.9800221975582686\n",
      "Min Samples Leaf: 9, Accuracy: 0.9811320754716981\n",
      "Min Samples Leaf: 10, Accuracy: 0.9822419533851277\n",
      "Min Samples Leaf: 11, Accuracy: 0.9822419533851277\n",
      "Min Samples Leaf: 12, Accuracy: 0.9822419533851277\n",
      "Min Samples Leaf: 13, Accuracy: 0.9822419533851277\n",
      "Min Samples Leaf: 14, Accuracy: 0.9800221975582686\n",
      "Min Samples Leaf: 15, Accuracy: 0.9800221975582686\n",
      "Min Samples Leaf: 16, Accuracy: 0.9800221975582686\n",
      "Min Samples Leaf: 17, Accuracy: 0.9800221975582686\n",
      "Min Samples Leaf: 18, Accuracy: 0.9800221975582686\n",
      "Min Samples Leaf: 19, Accuracy: 0.9800221975582686\n",
      "Min Samples Leaf: 20, Accuracy: 0.9800221975582686\n",
      "Min Samples Leaf: 21, Accuracy: 0.978912319644839\n",
      "Min Samples Leaf: 22, Accuracy: 0.978912319644839\n",
      "Min Samples Leaf: 23, Accuracy: 0.978912319644839\n",
      "Min Samples Leaf: 24, Accuracy: 0.97669256381798\n",
      "Min Samples Leaf: 25, Accuracy: 0.9778024417314095\n",
      "Min Samples Leaf: 26, Accuracy: 0.9778024417314095\n",
      "Min Samples Leaf: 27, Accuracy: 0.97669256381798\n",
      "Min Samples Leaf: 28, Accuracy: 0.97669256381798\n",
      "Min Samples Leaf: 29, Accuracy: 0.97669256381798\n",
      "Min Samples Leaf: 30, Accuracy: 0.9722530521642619\n"
     ]
    }
   ],
   "source": [
    "# Try different values for min_samples_leaf\n",
    "def min_samples_leaf_dt(min_samples_leaf:int = None):\n",
    "\tdt = DecisionTreeClassifier(random_state=1, min_samples_leaf=min_samples_leaf)\n",
    "\tdt.fit(X_train, y_train)\n",
    "\tpredictions = dt.predict(X_test)\n",
    "\taccuracy = accuracy_score(y_test, predictions)\n",
    "\tprint(f\"Min Samples Leaf: {min_samples_leaf}, Accuracy: {accuracy}\")\n",
    "\n",
    "for samples in range(30):\n",
    "\tmin_samples_leaf_dt(samples + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the value for ``min_samples_leaf`` which gives the best accuracy? __Write it down in the following cell (if several values give the maximum accuracy, just write one of them). (1 point)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best value for min_samples_leaf (integer)\n",
    "\n",
    "t21_min_sample_leaf = 4 #Accuracy: 0.9822419533851277"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other parameters can be tuned for this algorithm. We won't go further in this exercise, but all the parameters can be found in the documentation of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Cost Complexity Pruning (20 min) <a class=\"anchor\" id=\"t22\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same dataset we used for the rest of the exercise, we will compare cost complexity pruning with the previous approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('03-decision-tree_1.csv', na_values = '?')\n",
    "df['Direct'] = df['Direct'].astype('category')\n",
    "x = ['shiptype', 'Length', 'Breadth', 'SOG', 'TotalDistance', 'Duration']\n",
    "y = ['Direct']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[x], df[y], test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use here the algorithm ``Cost Complexity feature`` from the library ``sklearn`` and ``DesisionTreeClassifier``. \n",
    "\n",
    "__Write the code for the train and test sets and applying the algorithm (still with__ ``random_state = 1``__). Get the accuracy of your model with the function__ ``accuracy_score``.\n",
    "\n",
    "Note: used as we did before, the algorithm ``DecissionTreeClassifier`` returns a warning message about the expected format of the parameter ``y`` (in our case, ``y_train``). This is just a warning so it could be ignored, but to suppress it, we can replace ``y_train`` with ``y_train.values.ravel()``. This basically changes the type of the parameters ``y_train`` but has no influence on the model. To verify, you can try to compare the accuracy of both models, with and without the change.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the  libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to build a complete decision tree as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97669256381798\n"
     ]
    }
   ],
   "source": [
    "# Build the decision tree\n",
    "dt = DecisionTreeClassifier(random_state=1)\n",
    "dt.fit(X_train, y_train)\n",
    "predictions = dt.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having build the decision tree. We can apply cost complexitiy pruning by using the function _cost_complexity_pruning_path_ of the classifier. Input are the feature vector of the training dataset and the vector of the feature that should be predicted.\n",
    "The function returns a complex structure which we call __path__. You can get the alpha values by __path__['ccp_alphas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.00016195, 0.00018335, 0.00018508, 0.00024986,\n",
       "       0.0002568 , 0.00025779, 0.00025779, 0.0002644 , 0.0002644 ,\n",
       "       0.00026555, 0.00027068, 0.00027597, 0.00027752, 0.00027944,\n",
       "       0.00029613, 0.00036194, 0.00037016, 0.00037016, 0.00037016,\n",
       "       0.00040221, 0.00040886, 0.00041644, 0.00041644, 0.00041644,\n",
       "       0.00042084, 0.00046271, 0.00047593, 0.00047611, 0.00051496,\n",
       "       0.00056056, 0.00069406, 0.00083154, 0.00085524, 0.0008884 ,\n",
       "       0.00092039, 0.00112335, 0.00165648, 0.00169394, 0.00182045,\n",
       "       0.0018268 , 0.00209115, 0.00382123, 0.00484766, 0.1025496 ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the alpha array of Cost Complexity pruning\n",
    "path= dt.cost_complexity_pruning_path(X_train, y_train)\n",
    "alphas= path['ccp_alphas']\n",
    "alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCP Alpha: 0.0, Accuracy: 0.97669256381798\n",
      "CCP Alpha: 0.00016194706644456782, Accuracy: 0.97669256381798\n",
      "CCP Alpha: 0.00018335261995326227, Accuracy: 0.97669256381798\n",
      "CCP Alpha: 0.00018508236165093466, Accuracy: 0.97669256381798\n",
      "CCP Alpha: 0.0002498611882287619, Accuracy: 0.9778024417314095\n",
      "CCP Alpha: 0.000256801776790672, Accuracy: 0.9778024417314095\n",
      "CCP Alpha: 0.00025779328944237337, Accuracy: 0.9811320754716981\n",
      "CCP Alpha: 0.00025779328944237337, Accuracy: 0.9811320754716981\n",
      "CCP Alpha: 0.0002644033737870496, Accuracy: 0.9800221975582686\n",
      "CCP Alpha: 0.0002644033737870496, Accuracy: 0.9800221975582686\n",
      "CCP Alpha: 0.00026555295367308024, Accuracy: 0.9800221975582686\n",
      "CCP Alpha: 0.00027068295391449173, Accuracy: 0.9800221975582686\n",
      "CCP Alpha: 0.0002759710213902323, Accuracy: 0.9800221975582686\n",
      "CCP Alpha: 0.0002775219233173486, Accuracy: 0.9811320754716981\n",
      "CCP Alpha: 0.0002794399812656575, Accuracy: 0.9822419533851277\n",
      "CCP Alpha: 0.0002961317786414954, Accuracy: 0.9833518312985572\n",
      "CCP Alpha: 0.00036193884056182806, Accuracy: 0.9833518312985572\n",
      "CCP Alpha: 0.00037016472330186933, Accuracy: 0.9833518312985572\n",
      "CCP Alpha: 0.00037016472330186933, Accuracy: 0.9833518312985572\n",
      "CCP Alpha: 0.00037016472330186933, Accuracy: 0.9833518312985572\n",
      "CCP Alpha: 0.00040220610563475686, Accuracy: 0.9833518312985572\n",
      "CCP Alpha: 0.00040886376255615557, Accuracy: 0.9833518312985572\n",
      "CCP Alpha: 0.000416435313714603, Accuracy: 0.9833518312985572\n",
      "CCP Alpha: 0.000416435313714603, Accuracy: 0.9833518312985572\n",
      "CCP Alpha: 0.000416435313714603, Accuracy: 0.9833518312985572\n",
      "CCP Alpha: 0.0004208420366110536, Accuracy: 0.9833518312985572\n",
      "CCP Alpha: 0.0004627059041273367, Accuracy: 0.9833518312985572\n",
      "CCP Alpha: 0.00047592607281668925, Accuracy: 0.9833518312985572\n",
      "CCP Alpha: 0.00047611019573715876, Accuracy: 0.9844617092119867\n",
      "CCP Alpha: 0.0005149578147641556, Accuracy: 0.9833518312985572\n",
      "CCP Alpha: 0.0005605579131206944, Accuracy: 0.9833518312985572\n",
      "CCP Alpha: 0.000694058856191005, Accuracy: 0.9833518312985572\n",
      "CCP Alpha: 0.0008315422851207066, Accuracy: 0.9833518312985572\n",
      "CCP Alpha: 0.0008552432205958024, Accuracy: 0.9833518312985572\n",
      "CCP Alpha: 0.0008883953359244861, Accuracy: 0.9822419533851277\n",
      "CCP Alpha: 0.0009203881441527193, Accuracy: 0.9822419533851277\n",
      "CCP Alpha: 0.0011233496442862889, Accuracy: 0.9822419533851277\n",
      "CCP Alpha: 0.001656484111565638, Accuracy: 0.9811320754716981\n",
      "CCP Alpha: 0.0016939442813956977, Accuracy: 0.9755826859045506\n",
      "CCP Alpha: 0.0018204463478764532, Accuracy: 0.9755826859045506\n",
      "CCP Alpha: 0.00182679536654294, Accuracy: 0.974472807991121\n",
      "CCP Alpha: 0.002091145944560714, Accuracy: 0.9722530521642619\n",
      "CCP Alpha: 0.0038212282543890093, Accuracy: 0.9722530521642619\n",
      "CCP Alpha: 0.004847662490142204, Accuracy: 0.9722530521642619\n",
      "CCP Alpha: 0.10254959684576866, Accuracy: 0.9223085460599334\n"
     ]
    }
   ],
   "source": [
    "# Find the best alpha and tree. \n",
    "clfs = []\n",
    "accs = []\n",
    "\n",
    "def ccp_alpha_dt(ccp_alpha:float = 0):\n",
    "\tdt = DecisionTreeClassifier(random_state=1, ccp_alpha=ccp_alpha)\n",
    "\tdt.fit(X_train, y_train)\n",
    "\tpredictions = dt.predict(X_test)\n",
    "\taccuracy = accuracy_score(y_test, predictions)\n",
    "\tprint(f\"CCP Alpha: {ccp_alpha}, Accuracy: {accuracy}\")\n",
    "\tclfs.append(dt)\n",
    "\taccs.append(accuracy)\n",
    "\n",
    "\n",
    "\n",
    "for alpha in alphas:\n",
    "\tccp_alpha_dt(alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down the alpha and compare the tree (depth, number of nodes, accuracy) with the tree you found with max_depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparision\n",
    "\n",
    "best_idx = np.argmax(accs)\n",
    "best_tree = clfs[best_idx]\n",
    "\n",
    "t22_best_alpha = alphas[best_idx]\n",
    "t22_cost_complexity_accuray = accs[best_idx]\n",
    "t22_cost_complexity_depth = best_tree.get_depth()\n",
    "t22_cost_complexity_nodes=best_tree.tree_.node_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_What is the main difference between choosing max_depth and cost complexity pruning_\n",
    "\n",
    ": max_depth - cut the tree at a given depth even if it increases the error rate, simple to implement. but cost_complexity_pruning limits or cuts the tree based on actual contribution to accuracy , needs more computation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
